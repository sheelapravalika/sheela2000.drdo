{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheelapravalika/sheela2000.drdo/blob/main/cybersecurity_fixed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mtn6_M5gujFV",
        "outputId": "90a00510-871b-4655-8132-35fc575870d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.0)\n",
            "Collecting fsspec\n",
            "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U datasets huggingface_hub fsspec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555,
          "referenced_widgets": [
            "851c94baa67545d89dab1ae9caced098",
            "27aef7b47ec840d48d57fa04a8fbc031",
            "2a197e6bf4d440dba7847ef403079f33",
            "88f615b718fa49b582fb0b31d5d6b355",
            "9a027561012349798e380c3528ad89cd",
            "b368778764f945b5942fe8190c4021b7",
            "77b795520a2f4a0990e85b067e054a23",
            "8bf59df53d17456eb065cd1d3806f6ad",
            "d7307c8b86104501845836b343dc49e6",
            "b753ef38e849461b8bfaa5557559fc5e",
            "95ecd93956e5423188c2070b023bce55"
          ]
        },
        "id": "3BMS6Tz2vLTW",
        "outputId": "b7b41ac6-3f6f-4d15-8fff-24563d350bc3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "851c94baa67545d89dab1ae9caced098",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/34 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['No.', 'Time', 'Source', 'Destination', 'Protocol', 'Length', 'Info'],\n",
            "        num_rows: 151062\n",
            "    })\n",
            "})\n",
            "['No.', 'Time', 'Source', 'Destination', 'Protocol', 'Length', 'Info']\n",
            "{'No.': Value(dtype='int64', id=None), 'Time': Value(dtype='float64', id=None), 'Source': Value(dtype='string', id=None), 'Destination': Value(dtype='string', id=None), 'Protocol': Value(dtype='string', id=None), 'Length': Value(dtype='int64', id=None), 'Info': Value(dtype='string', id=None)}\n",
            "     No.        Time                                   Source  \\\n",
            "0  10001  375.057687                           192.168.239.25   \n",
            "1  10002  375.062327                        2600:1901:1:b05::   \n",
            "2  10003  375.062460  2409:40f4:100b:c1b6:b9fb:3ec3:5675:a236   \n",
            "3  10004  375.063494  2409:40f4:100b:c1b6:b9fb:3ec3:5675:a236   \n",
            "4  10005  375.067654                        2600:1901:1:b05::   \n",
            "\n",
            "                               Destination Protocol  Length  \\\n",
            "0                          239.255.255.250     SSDP     167   \n",
            "1  2409:40f4:100b:c1b6:b9fb:3ec3:5675:a236  TLSv1.3     749   \n",
            "2                        2600:1901:1:b05::      TCP      86   \n",
            "3                        2600:1901:1:c36::     QUIC    1292   \n",
            "4  2409:40f4:100b:c1b6:b9fb:3ec3:5675:a236  TLSv1.3     638   \n",
            "\n",
            "                                                Info  \n",
            "0                               M-SEARCH * HTTP/1.1   \n",
            "1                 Application Data, Application Data  \n",
            "2  60888  >  443 [ACK] Seq=1505 Ack=4066 Win=6361...  \n",
            "3     Protected Payload (KP0), DCID=d90bfc3231755645  \n",
            "4                                   Application Data  \n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset (example: NSL-KDD)\n",
        "dataset = load_dataset(\"onurkya7/NADW-network-attacks-dataset\")\n",
        "\n",
        "# Check the available splits (train/test)\n",
        "print(dataset)\n",
        "\n",
        "# Access the train data\n",
        "train_data = dataset['train']\n",
        "\n",
        "\n",
        "# Preview train data\n",
        "train_data_df = train_data.to_pandas()\n",
        "\n",
        "# ✅ FIX: Use `train_data` instead of `data`\n",
        "print(train_data.column_names)   # Shows the list of column names (features)\n",
        "print(train_data.features)       # Shows the datatype info of each column\n",
        "\n",
        "# Show top rows\n",
        "print(train_data_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo0DxXtVZHBu",
        "outputId": "7d803741-8125-4477-c097-c3ced766e3d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Scaled Features (X_scaled):\n",
            "[[-1.16247639 -1.13554995]\n",
            " [ 1.27872403  1.29777137]\n",
            " [-0.11624764 -0.16222142]]\n",
            "\n",
            "🏷️ Encoded Labels (y):\n",
            "[0, 1, 0]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "def load_and_preprocess(csv_path):\n",
        "    # Load the CSV file\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Drop rows with missing values\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Encode the 'Label' column (target variable)\n",
        "    label_encoder = LabelEncoder()\n",
        "    df['Label'] = label_encoder.fit_transform(df['Label'])\n",
        "\n",
        "    # Separate features and labels\n",
        "    X = df.drop('Label', axis=1)\n",
        "    y = df['Label']\n",
        "\n",
        "    # Normalize the features using StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    return X_scaled, y\n",
        "\n",
        "# ======= 🧪 Example: Run the function and print output =======\n",
        "\n",
        "# Create a sample CSV file for testing\n",
        "sample_data = {\n",
        "    'feature1': [0.5, 1.2, 0.8],\n",
        "    'feature2': [100, 150, 120],\n",
        "    'Label': ['Benign', 'Threat', 'Benign']\n",
        "}\n",
        "\n",
        "# Save as CSV\n",
        "df_sample = pd.DataFrame(sample_data)\n",
        "df_sample.to_csv(\"sample_dataset.csv\", index=False)\n",
        "\n",
        "# Call the function\n",
        "X_scaled, y = load_and_preprocess(\"sample_dataset.csv\")\n",
        "\n",
        "# Output results\n",
        "print(\"📊 Scaled Features (X_scaled):\")\n",
        "print(X_scaled)\n",
        "\n",
        "print(\"\\n🏷️ Encoded Labels (y):\")\n",
        "print(y.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyN4cEQCV5LN",
        "outputId": "7d803741-8125-4477-c097-c3ced766e3d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Scaled Features (X_scaled):\n",
            "[[-1.16247639 -1.13554995]\n",
            " [ 1.27872403  1.29777137]\n",
            " [-0.11624764 -0.16222142]]\n",
            "\n",
            "🏷️ Encoded Labels (y):\n",
            "[0, 1, 0]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "def load_and_preprocess(csv_path):\n",
        "    # Load the CSV file\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Drop rows with missing values\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Encode the 'Label' column (target variable)\n",
        "    label_encoder = LabelEncoder()\n",
        "    df['Label'] = label_encoder.fit_transform(df['Label'])\n",
        "\n",
        "    # Separate features and labels\n",
        "    X = df.drop('Label', axis=1)\n",
        "    y = df['Label']\n",
        "\n",
        "    # Normalize the features using StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    return X_scaled, y\n",
        "\n",
        "# ======= 🧪 Example: Run the function and print output =======\n",
        "\n",
        "# Create a sample CSV file for testing\n",
        "sample_data = {\n",
        "    'feature1': [0.5, 1.2, 0.8],\n",
        "    'feature2': [100, 150, 120],\n",
        "    'Label': ['Benign', 'Threat', 'Benign']\n",
        "}\n",
        "\n",
        "# Save as CSV\n",
        "df_sample = pd.DataFrame(sample_data)\n",
        "df_sample.to_csv(\"sample_dataset.csv\", index=False)\n",
        "\n",
        "# Call the function\n",
        "X_scaled, y = load_and_preprocess(\"sample_dataset.csv\")\n",
        "\n",
        "# Output results\n",
        "print(\"📊 Scaled Features (X_scaled):\")\n",
        "print(X_scaled)\n",
        "\n",
        "print(\"\\n🏷️ Encoded Labels (y):\")\n",
        "print(y.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irGGivnVWL0a",
        "outputId": "7cc84b5f-c7c7-42a4-b315-6813bec62998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Selected Feature Matrix:\n",
            "[[-1.45010473 -0.4472136 ]\n",
            " [ 1.25675744  1.34164079]\n",
            " [-0.29002095 -1.34164079]\n",
            " [ 0.48336824  0.4472136 ]]\n",
            "\n",
            "📌 Selected Feature Names:\n",
            "['feature1', 'feature3']\n",
            "\n",
            "📈 F-scores of All Features:\n",
            "feature1: 6.2308\n",
            "feature2: 4.5000\n",
            "feature3: 8.0000\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "def load_and_preprocess(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df.dropna(inplace=True)\n",
        "    label_encoder = LabelEncoder()\n",
        "    df['Label'] = label_encoder.fit_transform(df['Label'])\n",
        "    X = df.drop('Label', axis=1)\n",
        "    y = df['Label']\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    return X_scaled, y, X.columns\n",
        "\n",
        "def select_features(X, y, k=2):  # For example, select top 2 features\n",
        "    selector = SelectKBest(score_func=f_classif, k=k)\n",
        "    X_new = selector.fit_transform(X, y)\n",
        "    selected_indices = selector.get_support(indices=True)\n",
        "    scores = selector.scores_\n",
        "    return X_new, selected_indices, scores\n",
        "\n",
        "# ====== 🧪 Sample Data for Testing ======\n",
        "sample_data = {\n",
        "    'feature1': [0.5, 1.2, 0.8, 1.0],\n",
        "    'feature2': [100, 150, 120, 130],\n",
        "    'feature3': [20, 22, 19, 21],\n",
        "    'Label': ['Benign', 'Threat', 'Benign', 'Threat']\n",
        "}\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(sample_data)\n",
        "df.to_csv(\"sample_features.csv\", index=False)\n",
        "\n",
        "# Run preprocessing and feature selection\n",
        "X_scaled, y, columns = load_and_preprocess(\"sample_features.csv\")\n",
        "X_selected, indices, scores = select_features(X_scaled, y, k=2)\n",
        "\n",
        "# Output results\n",
        "print(\"✅ Selected Feature Matrix:\")\n",
        "print(X_selected)\n",
        "\n",
        "print(\"\\n📌 Selected Feature Names:\")\n",
        "print(columns[indices].tolist())\n",
        "\n",
        "print(\"\\n📈 F-scores of All Features:\")\n",
        "for i, col in enumerate(columns):\n",
        "    print(f\"{col}: {scores[i]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyYc-xzBXRvu",
        "outputId": "d52add9c-9692-4635-c83e-14d1cfba1874"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Accuracy: 1.0\n",
            "\n",
            "📊 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         1\n",
            "           1       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         2\n",
            "   macro avg       1.00      1.00      1.00         2\n",
            "weighted avg       1.00      1.00      1.00         2\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Step 1: Data Loading and Preprocessing\n",
        "def load_and_preprocess(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df.dropna(inplace=True)\n",
        "    le = LabelEncoder()\n",
        "    df['Label'] = le.fit_transform(df['Label'])\n",
        "    X = df.drop('Label', axis=1)\n",
        "    y = df['Label']\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    return X_scaled, y\n",
        "\n",
        "# Step 2: Feature Selection\n",
        "def select_features(X, y, k=2):\n",
        "    selector = SelectKBest(score_func=f_classif, k=k)\n",
        "    X_selected = selector.fit_transform(X, y)\n",
        "    return X_selected\n",
        "\n",
        "# Step 3: Model Training Function\n",
        "def train_model(X_train, y_train):\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "# Step 4: Simulate Dataset\n",
        "data = {\n",
        "    'feature1': [0.5, 1.2, 0.8, 1.0, 1.5, 0.9],\n",
        "    'feature2': [100, 150, 120, 130, 160, 110],\n",
        "    'feature3': [20, 22, 19, 21, 25, 23],\n",
        "    'Label': ['Benign', 'Threat', 'Benign', 'Threat', 'Threat', 'Benign']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv(\"sample_rf.csv\", index=False)\n",
        "\n",
        "# Step 5: Run Pipeline\n",
        "X, y = load_and_preprocess(\"sample_rf.csv\")\n",
        "X_selected = select_features(X, y, k=2)\n",
        "\n",
        "# Step 6: Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 7: Train the Model\n",
        "model = train_model(X_train, y_train)\n",
        "\n",
        "# Step 8: Evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"✅ Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\n📊 Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63c9nTiUXtWn",
        "outputId": "b9203c87-dc20-4a36-ccdd-97a0876c65d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Scaled Features (X_scaled):\n",
            "[[-1.16247639 -1.13554995 -0.9258201 ]\n",
            " [ 1.27872403  1.29777137  1.38873015]\n",
            " [-0.11624764 -0.16222142 -0.46291005]]\n",
            "\n",
            "🏷️ Encoded Labels (y):\n",
            "[0, 1, 0]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# Your provided function\n",
        "def load_and_preprocess(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df.dropna(inplace=True)\n",
        "    label_encoder = LabelEncoder()\n",
        "    df['Label'] = label_encoder.fit_transform(df['Label'])\n",
        "    X = df.drop('Label', axis=1)\n",
        "    y = df['Label']\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    return X_scaled, y\n",
        "\n",
        "# Step 1: Create and save a sample dataset\n",
        "data = {\n",
        "    'feature1': [0.5, 1.2, 0.8],\n",
        "    'feature2': [100, 150, 120],\n",
        "    'feature3': [20, 25, 21],\n",
        "    'Label': ['Benign', 'Threat', 'Benign']\n",
        "}\n",
        "df_sample = pd.DataFrame(data)\n",
        "csv_path = \"sample_input.csv\"\n",
        "df_sample.to_csv(csv_path, index=False)\n",
        "\n",
        "# Step 2: Call your function\n",
        "X_scaled, y = load_and_preprocess(csv_path)\n",
        "\n",
        "# Step 3: Print the output\n",
        "print(\"✅ Scaled Features (X_scaled):\")\n",
        "print(X_scaled)\n",
        "\n",
        "print(\"\\n🏷️ Encoded Labels (y):\")\n",
        "print(y.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UpUouI2YGgd",
        "outputId": "3b0c4e2d-3bcd-4280-d5a4-9b578209ae10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Model saved as 'rf_model.pkl'\n",
            " Model loaded successfully\n",
            "Accuracy on test data: 0.0\n",
            " Test predictions: [0, 0]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# === STEP 1: Save and Load Functions ===\n",
        "def save_model(model, filename='rf_model.pkl'):\n",
        "    joblib.dump(model, filename)\n",
        "\n",
        "def load_model(filename='rf_model.pkl'):\n",
        "    return joblib.load(filename)\n",
        "\n",
        "# === STEP 2: Data Setup ===\n",
        "data = {\n",
        "    'feature1': [0.5, 1.2, 0.8, 1.0],\n",
        "    'feature2': [100, 150, 120, 130],\n",
        "    'feature3': [20, 25, 22, 21],\n",
        "    'Label': ['Benign', 'Threat', 'Benign', 'Threat']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Encode and scale\n",
        "le = LabelEncoder()\n",
        "df['Label'] = le.fit_transform(df['Label'])  # Benign=0, Threat=1\n",
        "X = df.drop('Label', axis=1)\n",
        "y = df['Label']\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.5, random_state=42)\n",
        "\n",
        "# === STEP 3: Train a Model ===\n",
        "model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# === STEP 4: Save the Model ===\n",
        "save_model(model, 'rf_model.pkl')\n",
        "print(\" Model saved as 'rf_model.pkl'\")\n",
        "\n",
        "# === STEP 5: Load the Model and Predict ===\n",
        "loaded_model = load_model('rf_model.pkl')\n",
        "print(\" Model loaded successfully\")\n",
        "\n",
        "# Make prediction using loaded model\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# === STEP 6: Evaluate Output ===\n",
        "print(\"Accuracy on test data:\", accuracy_score(y_test, y_pred))\n",
        "print(\" Test predictions:\", y_pred.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxpuBH62YdtI",
        "outputId": "03b42d62-418b-4e9c-f43b-9a6743c5d67b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   feature1  feature2  feature3   Label\n",
            "0       0.5       100        20  Benign\n",
            "1       1.2       150        25  Threat\n",
            "2       0.8       120        22  Benign\n",
            "3       1.0       130        21  Threat\n",
            "📌 Initial Accuracy: 0.0\n",
            "   feature1  feature2  feature3   Label\n",
            "0       1.4       140        24  Threat\n",
            "1       0.6       110        19  Benign\n",
            "✅ Accuracy after retraining on new data: 1.0\n",
            "🔍 New Predictions: [1, 1]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# === STEP 1: Initial Data ===\n",
        "data = {\n",
        "    'feature1': [0.5, 1.2, 0.8, 1.0],\n",
        "    'feature2': [100, 150, 120, 130],\n",
        "    'feature3': [20, 25, 22, 21],\n",
        "    'Label': ['Benign', 'Threat', 'Benign', 'Threat']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "# Label encode and scale\n",
        "le = LabelEncoder()\n",
        "df['Label'] = le.fit_transform(df['Label'])  # Benign=0, Threat=1\n",
        "X = df.drop('Label', axis=1)\n",
        "y = df['Label']\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.5, random_state=42)\n",
        "\n",
        "# === STEP 2: Initial Model Training ===\n",
        "model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "initial_preds = model.predict(X_test)\n",
        "\n",
        "print(\"📌 Initial Accuracy:\", accuracy_score(y_test, initial_preds))\n",
        "\n",
        "# === STEP 3: New Data to Retrain ===\n",
        "new_data = {\n",
        "    'feature1': [1.4, 0.6],\n",
        "    'feature2': [140, 110],\n",
        "    'feature3': [24, 19],\n",
        "    'Label': ['Threat', 'Benign']\n",
        "}\n",
        "df_new = pd.DataFrame(new_data)\n",
        "print(df_new)\n",
        "df_new['Label'] = le.transform(df_new['Label'])\n",
        "X_new = scaler.transform(df_new.drop('Label', axis=1))\n",
        "y_new = df_new['Label']\n",
        "#print(X_new)\n",
        "#print(y_new)\n",
        "# === STEP 4: Retrain Function ===\n",
        "def retrain_model_with_new_data(model, X_new, y_new):\n",
        "    model.fit(X_new, y_new)\n",
        "    return model\n",
        "\n",
        "# Retrain the model\n",
        "model = retrain_model_with_new_data(model, X_new, y_new)\n",
        "\n",
        "# Evaluate after retraining (using test data)\n",
        "final_preds = model.predict(X_test)\n",
        "print(\"✅ Accuracy after retraining on new data:\", accuracy_score(y_test, final_preds))\n",
        "print(\"🔍 New Predictions:\", final_preds.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X87rWXQOzYlC"
      },
      "outputs": [],
      "source": [
        "import nbformat\n",
        "\n",
        "# Step 1: Upload your broken notebook again\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Replace this with the actual uploaded filename\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "# Step 2: Load and clean\n",
        "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "    nb = nbformat.read(f, as_version=4)\n",
        "\n",
        "# ✅ Step 3: Clean ONLY metadata.widgets (KEEP outputs and execution_count)\n",
        "for cell in nb.cells:\n",
        "    if 'metadata' in cell:\n",
        "        cell['metadata'] = {k: v for k, v in cell['metadata'].items() if k != 'widgets'}\n",
        "\n",
        "# ✅ Step 4: Clear notebook-level metadata.widgets (if present)\n",
        "if 'widgets' in nb.get('metadata', {}):\n",
        "    del nb['metadata']['widgets']\n",
        "\n",
        "# Step 5: Save cleaned notebook\n",
        "cleaned_filename = filename.replace('.ipynb', '_fixed.ipynb')\n",
        "with open(cleaned_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    nbformat.write(nb, f)\n",
        "\n",
        "print(f\"✅ Cleaned notebook saved (outputs preserved) as {cleaned_filename}\")\n",
        "files.download(cleaned_filename)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}